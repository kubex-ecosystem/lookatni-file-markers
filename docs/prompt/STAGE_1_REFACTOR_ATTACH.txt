# LookAtni Code - Gerado automaticamente
# Data: 2025-08-14T01:32:43.012Z
# Fonte: ./
# Total de arquivos: 6

/// docs/README.md ///

# gox\_mod.sh â€” Build Go modular, robust e publicÃ¡vel

Agora com **empacotamento automÃ¡tico** por sistema, **layout configurÃ¡vel** e **paralelismo controlado**.

## Novidades

* `--pack[=auto|zip|tar]` â†’ cria `.zip` no Windows e `.tar.gz` no restante (auto), ou forÃ§a um formato.
* `--layout flat|pkg` e `--name TEMPLATE` â†’ controle de diretÃ³rio/nomes. Placeholders: `{mod}`, `{pkg}`, `{os}`, `{arch}`, `{ext}`.
* `--jobs N` â†’ paraleliza builds de combinaÃ§Ãµes OS/ARCH e mÃºltiplos `main` (usa `xargs -P` quando disponÃ­vel). O compilador do Go jÃ¡ paraleliza por pacote; aqui paralelizamos a **matriz externa**.

## Exemplos

```bash
# Layout por pacote e empacotamento automÃ¡tico
./gox_mod.sh --all --layout pkg --pack

# Nome custom + 4 jobs
./gox_mod.sh --os linux,darwin --arch amd64,arm64 \
  --layout flat --name '{mod}-{pkg}-{os}-{arch}{ext}' --jobs 4

# Injetando versÃ£o/commit/data (seu cÃ³digo tiver variÃ¡veis)
BUILDINFO_PATH=main ./gox_mod.sh --all --pack
```

## DependÃªncias de empacotamento

* `tar` para `.tar.gz` (padrÃ£o em macOS/Linux/BSD)
* `zip` para `.zip` (opcional). Se ausentes, o build segue sem empacotar e loga aviso.

---

PrÃ³ximos passos (opcional): manifest JSON + checksums, assinatura (minisign/age) e workflow YAML de release com matrix.

/// gox_mod_final.sh ///
#!/usr/bin/env bash

# gox_mod.sh â€” modular, robust and "publishable" Go build
# License: MIT (add LICENSE to repo if desired)
# Requirements: bash 4+, Go 1.18+ (recommended), git

set -o errexit
set -o errtrace
set -o functrace
set -o nounset
set -o pipefail
shopt -s inherit_errexit

IFS=$'\n\t'

# ====== Defaults (override via ENV or flags) ======
: "${DIST_DIR:=dist}"
: "${MAKE_TARGET:=build-dev}"
: "${ENABLE_UPX:=0}"                 # 1 to compress with upx
: "${DEFAULT_OS:=}"                  # e.g.: linux
: "${DEFAULT_ARCH:=}"                # e.g.: amd64
: "${BUILD_TAGS:=}"                  # e.g.: netgo,osusergo
: "${ENABLE_RACE:=0}"                # -race on host
: "${VERBOSE:=0}"                    # -v, --verbose
: "${DEBUG:=0}"                      # -x
: "${LD_EXTRA:=}"                    # user extra ldflags

# If BUILDINFO_PATH='main' (or 'mod/internal/buildinfo'),
# injects -X <path>.Version/.Commit/.BuildDate with git info.
: "${BUILDINFO_PATH:=}"

# ====== Utils ======
# Color codes for logs
_SUCCESS="\033[0;32m"
_WARN="\033[0;33m"
_ERROR="\033[0;31m"
_INFO="\033[0;36m"
_NOTICE="\033[0;35m"
_NC="\033[0m"

clear_screen() {
  printf "\033[H\033[2J"
}
now_ms() {
  if command -v gdate >/dev/null 2>&1; then
    # coreutils (brew install coreutils)
    gdate +%s%3N
  elif command -v python3 >/dev/null 2>&1; then
    python3 - <<'PY'
import time
print(int(time.time()*1000))
PY
  elif command -v perl >/dev/null 2>&1; then
    perl -MTime::HiRes=time -e 'printf("%d\n", time()*1000)'
  elif command -v ruby >/dev/null 2>&1; then
    ruby -e 'puts (Time.now.to_f*1000).to_i'
  elif command -v node >/dev/null 2>&1; then
    node -e 'console.log(Date.now())'
  else
    # fallback aproximado (segundos -> ms)
    echo "$(( $(date +%s) * 1000 ))"
  fi
}
log() {
  local type="${1:-}"
  local message="${2:-}"
  local verbose="${3:-${VERBOSE:-0}}"
  verbose="${verbose:-${DEBUG:-0}}"

  case $type in
    question|_QUESTION|-q|-Q)
      if (( verbose )) || (( DEBUG )); then
        printf '%b[QUESTION]%b - %s - â“  %s: ' "$_NOTICE" "$_NC" "$(date +%H:%M:%S)" "$message" >&2
      fi
      ;;
    notice|_NOTICE|-n|-N)
      if (( verbose )) || (( DEBUG )); then
        printf '%b[NOTICE]%b - %s - ðŸ“  %s\n' "$_NOTICE" "$_NC" "$(date +%H:%M:%S)" "$message" >&2
      fi
      ;;
    info|_INFO|-i|-I)
      if (( verbose )) || (( DEBUG )); then
        printf '%b[INFO]%b - %s - â„¹ï¸  %s\n' "$_INFO" "$_NC" "$(date +%H:%M:%S)" "$message" >&2
      fi
      ;;
    warn|_WARN|-w|-W)
      if (( verbose )) || (( DEBUG )); then
        printf '%b[WARN]%b - %s - âš ï¸  %s\n' "$_WARN" "$_NC" "$(date +%H:%M:%S)" "$message" >&2
      fi
      ;;
    error|_ERROR|-e|-E)
      printf '%b[ERROR]%b - %s - âŒ  %s\n' "$_ERROR" "$_NC" "$(date +%H:%M:%S)" "$message" >&2
      ;;
    success|_SUCCESS|-s|-S)
      printf '%b[SUCCESS]%b - %s - âœ…  %s\n' "$_SUCCESS" "$_NC" "$(date +%H:%M:%S)" "$message" >&2
      ;;
    fatal|_FATAL|-f|-F)
      printf '%b[FATAL]%b ðŸ’€ - %s - %s\n' "$_FATAL" "$_NC" "$(date +%H:%M:%S)" "$message" >&2
      if (( verbose )) || (( DEBUG )); then
        printf '%b[FATAL]%b ðŸ’€  %s\n' "$_FATAL" "$_NC" "Exiting due to fatal error." >&2
      fi
      # clear_build_artifacts
      exit 1
      ;;
    *)
      if (( verbose )) || (( DEBUG )); then
        log "info" "$message" "$verbose"
      fi
      ;;
  esac
  return 0
}
die()  { log fatal "$*"; }
have() { command -v "${1:-}" >/dev/null 2>&1; }

usage() {

  cat <<'EOF'
Usage: ./gox_mod.sh [options] [PATH]

Options:
--all                   Build for default matrix (linux,darwin,windows x amd64,arm64)
--os OS1,OS2            List of GOOS (e.g.: linux,darwin)
--arch A1,A2            List of GOARCH (e.g.: amd64,arm64)
--tags TAGS             Build tags (e.g.: 'netgo,osusergo')
--race                  Enables -race on native build
--upx                   Compress with UPX (if installed)
-d, --debug             Debug mode (set -x)
-v, --verbose           Verbose (-v)
-h, --help              Help

Environment:
DIST_DIR, MAKE_TARGET, ENABLE_UPX, DEFAULT_OS, DEFAULT_ARCH,
BUILD_TAGS, ENABLE_RACE, VERBOSE, LD_EXTRA, BUILDINFO_PATH

Examples:
./gox_mod.sh
./gox_mod.sh --all
./gox_mod.sh --os linux --arch arm64 ./cmd/myapp
BUILDINFO_PATH=main LD_EXTRA="-s -w" ./gox_mod.sh
EOF

}

_START_TIME="$(now_ms)"

# ====== Args ======
ALL=0
declare -A GOPLT_MAP=()
declare -a ARG_GOOS_LIST=()
declare -a ARG_GOARCH_LIST=()
TARGET_PATH=""

# shellcheck disable=SC2015
parse_args() {
  while (("$#")); do
    case "${1:-}" in
      --all) ALL=1 ;;
      --os) shift; IFS=',' read -r -a ARG_GOOS_LIST <<< "${1:-}";;
      --arch) shift; IFS=',' read -r -a ARG_GOARCH_LIST <<< "${1:-}";;
      --tags) shift; BUILD_TAGS="${1:-}";;
      --race) ENABLE_RACE=1 ;;
      --upx) ENABLE_UPX=1 ;;
      -d|--debug) DEBUG=1;;
      -v|--verbose) VERBOSE=1 ;;
      -h|--help) usage; exit 0 ;;
      *) TARGET_PATH="${1:-}";;
    esac
    shift || true
  done
  (( DEBUG )) && set -x || true
  (( VERBOSE )) && set -v || true
}

# ====== Project / module / git ======
PROJECT_ROOT=""
MOD_NAME=""
GIT_TAG=""
GIT_COMMIT=""
GIT_DATE=""

detect_project_root() {
  PROJECT_ROOT="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
  cd "${PROJECT_ROOT}"
}

read_module_name() {
  if [[ -f go.mod ]]; then
    MOD_NAME="$(awk '/^module /{print $2}' go.mod | awk -F'/' '{print $NF}')"
  fi
  [[ -n "${MOD_NAME}" ]] || MOD_NAME="$(basename "${PROJECT_ROOT}")"
}

git_info() {
  if have git && git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
    GIT_TAG="$(git describe --tags --dirty --always 2>/dev/null || true)"
    GIT_COMMIT="$(git rev-parse --short HEAD 2>/dev/null || true)"
    GIT_DATE="$(git show -s --format=%cd --date=format:%Y-%m-%d 2>/dev/null || date +%Y-%m-%d)"
    SOURCE_DATE_EPOCH="$(git log -1 --pretty=%ct 2>/dev/null || date +%s)"; export SOURCE_DATE_EPOCH
  else
    GIT_TAG="dev"; GIT_COMMIT="none"; GIT_DATE="$(date +%Y-%m-%d)"
    SOURCE_DATE_EPOCH="$(date +%s)"; export SOURCE_DATE_EPOCH
  fi
}

# ====== Discover 'main' packages ======
discover_mains() {
  local path="./..."
  if [[ -n "${TARGET_PATH}" ]]; then
    local rp; rp="$(realpath "${TARGET_PATH}")"
    if [[ -d "${rp}" ]]; then
      path="${rp}/..."
    else
      path="$(dirname "${rp}")/..."
    fi
  fi
  mapfile -t MAIN_DIRS < <(go list -f '{{if eq .Name "main"}}{{.Dir}}{{end}}' "${path}" | awk 'NF')
  (( ${#MAIN_DIRS[@]} )) || die "No 'main' package found."
}

# ====== GOOS/GOARCH Matrix ======
compute_matrix() {
  if (( ALL )); then
    GOPLT_MAP[linux]="(amd64)"
    GOPLT_MAP[darwin]="(amd64 arm64)"
    GOPLT_MAP[windows]="(amd64 386)"
    ARG_GOOS_LIST=(linux darwin windows)
    ARG_GOARCH_LIST=(amd64 arm64 386)
    return 0
  fi
  if [[ ${#ARG_GOOS_LIST[@]} -eq 0 ]]; then
    # shellcheck disable=SC2207
    if [[ -n "${DEFAULT_OS}" ]]; then
      ARG_GOOS_LIST=("${DEFAULT_OS}");
      for _GOENVOS in "${ARG_GOOS_LIST[@]}"; do
        GOPLT_MAP["${_GOENVOS}"]="(amd64)";
      done
    else
      ARG_GOOS_LIST=( $(go env GOOS) );
      for os in "${!ARG_GOOS_LIST[@]}"; do
        GOPLT_MAP[${ARG_GOOS_LIST[$os]}]="( $(go env GOARCH || echo amd64) )";
      done
    fi
  fi
  if [[ ${#ARG_GOARCH_LIST[@]} -eq 0 ]]; then
    # shellcheck disable=SC2207
    if [[ -n "${DEFAULT_ARCH}" ]]; then
      ARG_GOARCH_LIST=("${DEFAULT_ARCH}");
      for _GOENVARCH in "${ARG_GOARCH_LIST[@]}"; do
        for os in "${!ARG_GOOS_LIST[@]}"; do
          if [[ -z "${GOPLT_MAP[${ARG_GOOS_LIST[${os:-}]}]}" ]]; then
            GOPLT_MAP["${ARG_GOOS_LIST[${os:-}]}"]="(${_GOENVARCH})";
          else
            if ! [[ " ${GOPLT_MAP[${ARG_GOOS_LIST[${os:-}]}]} " == *" ${_GOENVARCH} "* ]]; then
              GOPLT_MAP["${ARG_GOOS_LIST[${os:-}]}"]+=" ${_GOENVARCH}";
            fi
          fi
        done
      done
    else
      ARG_GOARCH_LIST=( $(go env GOARCH) );
      for _GOENVARCH in "${ARG_GOARCH_LIST[@]}"; do
        for os in "${!ARG_GOOS_LIST[@]}"; do
          if [[ -z "${GOPLT_MAP[${ARG_GOOS_LIST[${os:-}]}]}" ]]; then
            GOPLT_MAP["${ARG_GOOS_LIST[${os:-}]}"]="(${_GOENVARCH})";
          else
            if ! [[ " ${GOPLT_MAP[${ARG_GOOS_LIST[${os:-}]}]} " == *" ${_GOENVARCH} "* ]]; then
              GOPLT_MAP["${ARG_GOOS_LIST[${os:-}]}"]+=" ${_GOENVARCH}";
            fi
          fi
        done
      done
    fi
  fi
}

# ====== Detect build flag support ======
supports_build_flag() {
  # e.g.: supports_build_flag buildvcs
  go help build 2>/dev/null | grep -q -- "-$1\b"
}

# ====== Build argument assembly ======
declare -a BUILD_ARGS=()

build_flags() {
  local os="${1:-}"
  local arch="${2:-}"
  BUILD_ARGS=()  # reset

  if supports_build_flag trimpath; then
    BUILD_ARGS+=(-trimpath)
  fi
  if supports_build_flag buildvcs; then
    BUILD_ARGS+=(-buildvcs)
  fi

  if (( ENABLE_RACE )); then
    local host_os;   host_os="$(uname -s | tr '[:upper:]' '[:lower:]')"
    local host_arch; host_arch="$(uname -m)"
    if [[ "$os" == "$host_os" && "$arch" == "$host_arch" ]]; then
      BUILD_ARGS+=(-race)
    fi
  fi

  if [[ -n "${BUILD_TAGS}" ]]; then
    BUILD_ARGS+=(-tags "${BUILD_TAGS}")
  fi

  local ld_str="-s -w"
  [[ -n "${LD_EXTRA}" ]] && ld_str="${ld_str} ${LD_EXTRA}"
  if [[ -n "${BUILDINFO_PATH}" ]]; then
    ld_str="${ld_str} -X ${BUILDINFO_PATH}.Version=${GIT_TAG}"
    ld_str="${ld_str} -X ${BUILDINFO_PATH}.Commit=${GIT_COMMIT}"
    ld_str="${ld_str} -X ${BUILDINFO_PATH}.BuildDate=${GIT_DATE}"
  fi
  BUILD_ARGS+=(-ldflags "${ld_str}")
}

# ====== UPX ======
maybe_upx() {
  local bin="${1:-}"
  (( ENABLE_UPX )) || return 0
  have upx || { log warn "UPX not found; continuing without compression."; return 0; }
  log notice "UPX: compressing '${bin}'"
  upx "$bin" --force-overwrite --lzma --no-progress --no-color -qqq || log info "UPX failed (ignoring)."
}

# ====== Build ======
build_one() {
  local _dir="${1:-.}"
  local _os="${2:-$(uname -s | tr '[:upper:]' '[:lower:]')}"
  local _arch="${3:-$(uname -m)}"
  local _ext="";
  [[ "${_os}" == "windows" ]] && _ext=".exe"

  local _pkg_name;
  _pkg_name="$(basename "${_dir}")"

  local _bin_name="${MOD_NAME}_${_os}_${_arch}${_ext:-}"
  local _out_dir="${DIST_DIR}" # /${_pkg_name}"

  mkdir -p "${_out_dir}"

  local _out_bin="${_out_dir}/${_bin_name}"

  log info "Building ${_pkg_name} -> ${_out_bin}"
  build_flags "${_os}" "${_arch}"

  GOOS="${_os}" GOARCH="${_arch}" \
  go mod tidy > /dev/null || {
    log warn "go mod tidy failed (ignoring)."
    return 1
  }
  GOOS="${_os}" GOARCH="${_arch}" \
  go build \
  "${BUILD_ARGS[@]}" \
  -o "${_out_bin}" "${_dir}"

  maybe_upx "${_out_bin}"
  log success "OK: ${_out_bin}"
}

build_all() {
  mkdir -p "${DIST_DIR}"

  if [[ -f Makefile ]]; then
    log info "Makefile detected. Running target '${MAKE_TARGET}'..."
    # shellcheck disable=SC1007
    if MAKEFLAGS= FORCE=y make "${MAKE_TARGET}"; then
      log success "Build via Makefile completed."
      return 0
    else
      log error "Target '${MAKE_TARGET}' failed. Fallback to 'go build'."
    fi
  fi

  for __os in "${!GOPLT_MAP[@]}"; do
    local arch_list
    GOPLT_MAP[$__os]=${GOPLT_MAP[${__os}]//\(/}
    GOPLT_MAP[${__os}]=${GOPLT_MAP[${__os}]//\)/}
    IFS=' ' read -r -a arch_list <<< "${GOPLT_MAP[${__os}]}"
    for __arch in "${arch_list[@]}"; do
      for __dir in "${MAIN_DIRS[@]}"; do
        build_one "${__dir}" "${__os}" "${__arch}"
      done
    done
  done
}

# ====== Performance measurement ======
measure_performance() {
  local _EXIT_CODE="$?"
  _EXIT_CODE="${_EXIT_CODE:-${1:-}}"
  _EXIT_CODE="${_EXIT_CODE:-0}"

  # Measure total duration in milliseconds
  _END_TIME="$(now_ms)"
  _DURATION=$(( _END_TIME - _START_TIME ))

  if (( _DURATION < 1000 )); then
    log notice "Total duration: ${_DURATION} ms"
  else
    log notice "Total duration: $(( _DURATION / 60000 )) min $(( (_DURATION / 1000) % 60 )) sec $(( _DURATION % 1000 )) ms"
  fi

  if (( _EXIT_CODE == 0 )); then
    log success "All builds succeeded."
  else
    log error "Builds completed with errors (exit code: ${_EXIT_CODE})."
  fi
}

# ====== Main ======
main() {
  parse_args "$@" || die "Error parsing arguments."
  have go || die "Go not found in PATH."
  detect_project_root || die "Could not detect project directory."
  read_module_name || die "Could not read module name."
  git_info || true
  discover_mains || die "Could not discover 'main' packages."
  compute_matrix || die "Error computing GOOS/GOARCH matrix."
  build_all || die "Build failed."
  measure_performance $? || true
  return 0
}

main "$@"

/// md_to_html.go ///
package main

import (
	"embed"
	"fmt"
	"io/fs"
	"os"
	"path/filepath"
	"strings"
	"time"

	"html/template"

	"github.com/gomarkdown/markdown"
	"github.com/gomarkdown/markdown/html"
	"github.com/gomarkdown/markdown/parser"
)

type mdFileContainer struct {
	ReadFile func(name string) ([]byte, error)
	FileInfo *fs.FileInfo
	Content  []byte
}

type FileInfo struct {
	FileName    string
	Title       string
	Description string
	Size        string
	Icon        string
}

type IndexData struct {
	Files       []FileInfo
	FileCount   int
	TotalSize   string
	GeneratedAt string
}

//go:embed all:tests/interview_*.md
var mdEmbeddedFilesList embed.FS

//go:embed tests/template.html.tmpl
var templateContentByteArr []byte

//go:embed tests/index.html.tmpl
var indexTemplateByteArr []byte

var (
	// mdFiles is initialized with the embedded files
	mdFiles = make(map[string]*mdFileContainer)

	_ = mdFiles
	_ = mdEmbeddedFilesList
)

func main() {
	// Initialize the embedded file system
	mdFilesList, err := mdEmbeddedFilesList.ReadDir("tests")
	if err != nil {
		fmt.Printf("Error reading embedded files: %v\n", err)
		os.Exit(1)
	}

	// Collect information about generated files for index
	var generatedFiles []FileInfo
	var totalSizeBytes int64

	// Get the target directory
	currentDir, err := os.Getwd()
	if err != nil {
		fmt.Printf("Error getting current directory: %v\n", err)
		os.Exit(1)
	}
	outputPath := filepath.Join(currentDir, "output", "interviews")

	for _, file := range mdFilesList {
		if !file.IsDir() && strings.HasPrefix(file.Name(), "interview_") && strings.HasSuffix(file.Name(), ".md") {
			// Read the file info
			fileInfo, err := file.Info()
			if err != nil {
				fmt.Printf("Error getting file info for %s: %v\n", file.Name(), err)
				continue
			}

			mdFiles[file.Name()] = &mdFileContainer{
				ReadFile: func(name string) ([]byte, error) {
					return mdEmbeddedFilesList.ReadFile(filepath.Join("tests", name))
				},
				FileInfo: &fileInfo,
				Content:  make([]byte, 0),
			}
			if fileInfo.IsDir() || !strings.HasSuffix(file.Name(), ".md") {
				fmt.Printf("Skipping %s: not a valid source entry\n", file.Name())
				continue
			}

			mdFiles[file.Name()] = &mdFileContainer{
				ReadFile: func(name string) ([]byte, error) {
					return mdEmbeddedFilesList.ReadFile(filepath.Join("tests", name))
				},
				FileInfo: &fileInfo,
				Content:  make([]byte, 0),
			}

			containerFile := mdFiles[file.Name()]
			containerFile.Content, err = mdFiles[file.Name()].ReadFile(file.Name())
			if err != nil {
				fmt.Printf("Error reading %s: %v\n", file.Name(), err)
				continue
			}

			htmlFileInfo, err := convertMarkdownToHTML(file.Name(), containerFile.Content)
			if err != nil {
				fmt.Printf("Error converting %s to HTML: %v\n", file.Name(), err)
			} else {
				generatedFiles = append(generatedFiles, htmlFileInfo)
				// Get file size for total calculation
				if stat, err := os.Stat(filepath.Join(outputPath, htmlFileInfo.FileName)); err == nil {
					totalSizeBytes += stat.Size()
				}
			}
		}
	}

	// Generate index.html after all files are processed
	if err := generateIndex(generatedFiles, totalSizeBytes); err != nil {
		fmt.Printf("Error generating index: %v\n", err)
	}
}

func convertMarkdownToHTML(mdFileTitle string, mdFileContent []byte) (FileInfo, error) {
	// Get the target directory
	currentDir, err := os.Getwd()
	if err != nil {
		fmt.Println("Error getting current directory:", err)
		os.Exit(1)
	}

	outputPath := filepath.Join(currentDir, "output", "interviews")
	if err := os.MkdirAll(outputPath, 0755); err != nil {
		fmt.Println("Error creating output directory:", err)
		os.Exit(1)
	}

	htmlFilePath := filepath.Join(
		outputPath,
		fmt.Sprintf(
			"./%s_view.html",
			strings.TrimSuffix(strings.TrimPrefix(mdFileTitle, "interview_"), ".md"),
		),
	)

	if len(mdFileContent) > 0 {
		fmt.Println("Converting", mdFileTitle, "to HTML at", htmlFilePath, ". (", len(mdFileContent), "bytes )")

		// The template content is embedded in the binary
		// And it has the simple structure bellow:
		// {{ .Content | markdown }}

		// Parse the template content
		var templateRenderer = template.Must(
			template.New("markdown").
				Option("missingkey=zero").
				Parse(string(templateContentByteArr)),
		)

		// Convert markdown to HTML first
		extensions := parser.CommonExtensions | parser.AutoHeadingIDs
		p := parser.NewWithExtensions(extensions)
		doc := p.Parse(mdFileContent)

		htmlFlags := html.CommonFlags | html.HrefTargetBlank
		opts := html.RendererOptions{Flags: htmlFlags}
		renderer := html.NewRenderer(opts)

		htmlContent := markdown.Render(doc, renderer)

		// Read the Markdown file content
		var ioWriter = strings.Builder{}

		// Execute the template with the HTML content
		if err := templateRenderer.Execute(&ioWriter, template.HTML(htmlContent)); err != nil {
			fmt.Println("Error executing template:", err)
			return FileInfo{}, err
		}

		// Convert the template output to final HTML
		finalHTML := []byte(ioWriter.String())

		// Write the HTML file
		if err = os.WriteFile(htmlFilePath, finalHTML, 0644); err != nil {
			fmt.Println("Error writing HTML file:", err)
			return FileInfo{}, err
		}

		fmt.Printf("HTML file %s generated successfully.\n", htmlFilePath)

		// Create FileInfo for index
		htmlFileName := filepath.Base(htmlFilePath)
		fileInfo := FileInfo{
			FileName:    htmlFileName,
			Title:       generateTitle(mdFileTitle),
			Description: generateDescription(mdFileTitle),
			Size:        fmt.Sprintf("%.1f", float64(len(finalHTML))/1024),
			Icon:        getFileIcon(mdFileTitle),
		}

		return fileInfo, nil
	} else {
		fmt.Println("No HTML content generated.")
		return FileInfo{}, fmt.Errorf("no content to convert")
	}
}

// generateTitle creates a human-readable title from the markdown filename
func generateTitle(mdFileName string) string {
	// Remove "interview_" prefix and ".md" suffix
	title := strings.TrimPrefix(mdFileName, "interview_")
	title = strings.TrimSuffix(title, ".md")

	// Convert underscores to spaces and capitalize
	title = strings.ReplaceAll(title, "_", " ")
	words := strings.Fields(title)
	for i, word := range words {
		if len(word) > 0 {
			words[i] = strings.ToUpper(word[:1]) + word[1:]
		}
	}

	return strings.Join(words, " ")
}

// generateDescription creates a description based on the file type
func generateDescription(mdFileName string) string {
	if strings.Contains(mdFileName, "test") {
		return "VersÃ£o de teste com perguntas e respostas simuladas para prÃ¡tica."
	} else if strings.Contains(mdFileName, "v2") {
		return "VersÃ£o refinada e polida com respostas mais elaboradas."
	} else {
		return "Documento principal de preparaÃ§Ã£o com todas as seÃ§Ãµes essenciais."
	}
}

// getFileIcon returns an emoji icon based on the file type
func getFileIcon(mdFileName string) string {
	if strings.Contains(mdFileName, "test") {
		return "ðŸ§ª"
	} else if strings.Contains(mdFileName, "v2") {
		return "âœ¨"
	} else {
		return "ðŸ“‹"
	}
}

// generateIndex creates the index.html file with links to all generated files
func generateIndex(files []FileInfo, totalSizeBytes int64) error {
	currentDir, err := os.Getwd()
	if err != nil {
		return fmt.Errorf("error getting current directory: %v", err)
	}

	outputPath := filepath.Join(currentDir, "output", "interviews")
	indexPath := filepath.Join(outputPath, "index.html")

	// Prepare data for template
	indexData := IndexData{
		Files:       files,
		FileCount:   len(files),
		TotalSize:   fmt.Sprintf("%.1f", float64(totalSizeBytes)/1024),
		GeneratedAt: time.Now().Format("02/01/2006 Ã s 15:04"),
	}

	// Parse the index template
	indexTemplate := template.Must(
		template.New("index").Parse(string(indexTemplateByteArr)),
	)

	// Execute template
	var indexWriter strings.Builder
	if err := indexTemplate.Execute(&indexWriter, indexData); err != nil {
		return fmt.Errorf("error executing index template: %v", err)
	}

	// Write index.html
	if err := os.WriteFile(indexPath, []byte(indexWriter.String()), 0644); err != nil {
		return fmt.Errorf("error writing index file: %v", err)
	}

	fmt.Printf("Index file %s generated successfully with %d files.\n", indexPath, len(files))
	return nil
}

/// tests/interview_preparation_script.md ///
# Lean Layer â€“ Data Engineer Interview Prep

## Tabela de ConteÃºdo

- [ApresentaÃ§Ã£o inicial](#apresentaÃ§Ã£o-inicial)
- [Follow-up do case Texaco](#follow-up-do-case-texaco)
- [Perguntas clÃ¡ssicas para Data Engineer](#perguntas-clÃ¡ssicas-para-data-engineer)
- [Plano de emergÃªncia](#plano-de-emergÃªncia)
- [SimulaÃ§Ãµes de Respostas](#simulaÃ§Ãµes-de-respostas)
- [Perguntas para o Entrevistador](#perguntas-para-o-entrevistador)
- [Resumo de Ferramentas e Tecnologias](#resumo-de-ferramentas-e-tecnologias)
- [Checklist de PreparaÃ§Ã£o](#checklist-de-preparaÃ§Ã£o)

---

## ApresentaÃ§Ã£o inicial

**Pergunta:**
*"Can you tell me a bit about yourself and your background as a Data Engineer?"*
*(Use o case Texaco como base)*

---

## Follow-up do case Texaco

**Q1.** *"What was the most challenging technical problem you faced in this project?"*

**Q2.** *"How did you ensure data quality and integrity?"*

**Q3.** *"If you were to rebuild this system today, what would you change?"*

**Q4.** *"How did you handle integrating multiple data sources?"*

**Q5.** *"Did you work alone or with a team, and how did you manage communication?"*

---

## Perguntas clÃ¡ssicas para Data Engineer

**Q6.** *"Tell me about a time when you improved a data pipeline or ETL process."*

**Q7.** *"How do you approach designing a data model for scalability?"*

**Q8.** *"Can you describe a situation where you had to deal with incomplete or inconsistent data?"*

**Q9.** *"Whatâ€™s your process for translating business requirements into technical solutions?"*

**Q10.** *"How do you stay updated with new data engineering tools and technologies?"*

---

## Plano de emergÃªncia

- **Se sentir inseguro logo no comeÃ§o**
*"While I can communicate in English, I donâ€™t have daily conversation practice, so sometimes I need an extra second to express an idea. Thatâ€™s why Iâ€™ve been taking structured speaking lessons to improve my fluency, and Iâ€™m already seeing progress."*

- **Se travar no meio da resposta**
*"Sorry, I just need a quick moment to phrase this correctly â€” English is not my first language, but Iâ€™m actively improving it. I prefer to take a second rather than give you the wrong answer."*

- **Se perguntarem sobre o idioma diretamente**
*"I believe communication is key, and thatâ€™s why Iâ€™m investing in my English speaking skills through a dedicated course. Iâ€™m confident my technical expertise and problem-solving abilities will allow me to deliver results while I continue improving my fluency."*

---

## SimulaÃ§Ãµes de Respostas

- **Q1:** *"What was the most challenging technical problem you faced in this project?"*
*Resposta simulada:* *"One of the most challenging problems was integrating legacy systems with modern cloud-based solutions. I had to design a custom ETL pipeline to ensure seamless data flow."*

- **Q6:** *"Tell me about a time when you improved a data pipeline or ETL process."*
*Resposta simulada:* *"In a previous project, I optimized an ETL pipeline by implementing parallel processing, reducing data processing time by 40%."*

---

## Perguntas para o Entrevistador

- *"What are the main challenges the team is currently facing?"*
- *"How does Lean Layers approach innovation in data engineering?"*
- *"What tools and technologies does the team use most frequently?"*

---

## Resumo de Ferramentas e Tecnologias

- **Ferramentas:** Apache Spark, Airflow, Tableau, Power BI
- **Linguagens:** Python, SQL
- **Cloud:** AWS, Azure

---

## Checklist de PreparaÃ§Ã£o

- [ ] Revisar o case Texaco
- [ ] Praticar respostas simuladas
- [ ] Estudar ferramentas e tecnologias mencionadas
- [ ] Preparar perguntas para o entrevistador
- [ ] Revisar plano de emergÃªncia para comunicaÃ§Ã£o em inglÃªs

/// tests/interview_preparation_script_test.md ///
# Lean Layer â€“ Data Engineer Interview Prep

## Tabela de ConteÃºdo

- [ApresentaÃ§Ã£o inicial](#apresentaÃ§Ã£o-inicial)
- [Follow-up do case Texaco](#follow-up-do-case-texaco)
- [Perguntas clÃ¡ssicas para Data Engineer](#perguntas-clÃ¡ssicas-para-data-engineer)
- [Plano de emergÃªncia](#plano-de-emergÃªncia)
- [SimulaÃ§Ãµes de Respostas](#simulaÃ§Ãµes-de-respostas)
- [Perguntas para o Entrevistador](#perguntas-para-o-entrevistador)
- [Resumo de Ferramentas e Tecnologias](#resumo-de-ferramentas-e-tecnologias)
- [Checklist de PreparaÃ§Ã£o](#checklist-de-preparaÃ§Ã£o)

---

## ApresentaÃ§Ã£o inicial

**Pergunta:**
*"Can you tell me a bit about yourself and your background as a Data Engineer?"*
*(Use o case Texaco como base)*

---

## Follow-up do case Texaco

**Q1.** *"What was the most challenging technical problem you faced in this project?"*

**Q2.** *"How did you ensure data quality and integrity?"*

**Q3.** *"If you were to rebuild this system today, what would you change?"*

**Q4.** *"How did you handle integrating multiple data sources?"*

**Q5.** *"Did you work alone or with a team, and how did you manage communication?"*

---

## Perguntas clÃ¡ssicas para Data Engineer

**Q6.** *"Tell me about a time when you improved a data pipeline or ETL process."*

**Q7.** *"How do you approach designing a data model for scalability?"*

**Q8.** *"Can you describe a situation where you had to deal with incomplete or inconsistent data?"*

**Q9.** *"Whatâ€™s your process for translating business requirements into technical solutions?"*

**Q10.** *"How do you stay updated with new data engineering tools and technologies?"*

---

## Plano de emergÃªncia

- **Se sentir inseguro logo no comeÃ§o**
*"While I can communicate in English, I donâ€™t have daily conversation practice, so sometimes I need an extra second to express an idea. Thatâ€™s why Iâ€™ve been taking structured speaking lessons to improve my fluency, and Iâ€™m already seeing progress."*

- **Se travar no meio da resposta**
*"Sorry, I just need a quick moment to phrase this correctly â€” English is not my first language, but Iâ€™m actively improving it. I prefer to take a second rather than give you the wrong answer."*

- **Se perguntarem sobre o idioma diretamente**
*"I believe communication is key, and thatâ€™s why Iâ€™m investing in my English speaking skills through a dedicated course. Iâ€™m confident my technical expertise and problem-solving abilities will allow me to deliver results while I continue improving my fluency."*

---

## SimulaÃ§Ãµes de Respostas

- **Q1:** *"What was the most challenging technical problem you faced in this project?"*
*Resposta simulada:* *"One of the most challenging problems was integrating legacy systems with modern cloud-based solutions. I had to design a custom ETL pipeline to ensure seamless data flow."*

- **Q6:** *"Tell me about a time when you improved a data pipeline or ETL process."*
*Resposta simulada:* *"In a previous project, I optimized an ETL pipeline by implementing parallel processing, reducing data processing time by 40%."*

---

## Perguntas para o Entrevistador

- *"What are the main challenges the team is currently facing?"*
- *"How does Lean Layers approach innovation in data engineering?"*
- *"What tools and technologies does the team use most frequently?"*

---

## Resumo de Ferramentas e Tecnologias

- **Ferramentas:** Apache Spark, Airflow, Tableau, Power BI
- **Linguagens:** Python, SQL
- **Cloud:** AWS, Azure

---

## Checklist de PreparaÃ§Ã£o

- [ ] Revisar o case Texaco
- [ ] Praticar respostas simuladas
- [ ] Estudar ferramentas e tecnologias mencionadas
- [ ] Preparar perguntas para o entrevistador
- [ ] Revisar plano de emergÃªncia para comunicaÃ§Ã£o em inglÃªs

/// tests/interview_preparation_script_v2.md ///

# Lean Layer â€“ Data Engineer Interview Prep (Refined Version)

## 1. Initial Presentation
**Question:**  
*"Can you tell me a bit about yourself and your background as a Data Engineer?"*

**Polished Answer:**  
"I am a Data Engineer with strong experience in designing and maintaining scalable data solutions.  
Seven years ago, I developed a complete data processing and reporting system for a major Texaco distributor in Brazil.  
I designed data consolidation models, integrated multiple sources, and built a modular, dynamic architecture that still powers the companyâ€™s commission system today â€” for a business averaging over 18 million reais in monthly revenue.  
My focus has always been on building reliable, maintainable data solutions that generate measurable business impact, which aligns closely with Lean Layerâ€™s mission."

---

## 2. Follow-up Questions for the Texaco Case

**Q1.** *"What was the most challenging technical problem you faced in this project?"*  
"The biggest challenge was integrating multiple data sources with different formats and quality levels.  
I solved it by creating an ETL pipeline with strict validation rules and transformation steps, ensuring all incoming data matched a unified schema before processing."

**Q2.** *"How did you ensure data quality and integrity?"*  
"I implemented automated validation at every stage of the pipeline â€” from raw ingestion to final reporting â€” including duplicate checks, null handling, and business-rule-based calculations.  
This reduced data errors to nearly zero."

**Q3.** *"If you were to rebuild this system today, what would you change?"*  
"I would migrate the architecture to a cloud-based stack with managed ETL services and modern BI tools.  
This would reduce maintenance overhead and make scaling easier while keeping the stability of the current logic."

**Q4.** *"How did you handle integrating multiple data sources?"*  
"I used a modular approach, creating connectors for each source that fed into a common processing layer.  
This allowed me to swap or update a connector without affecting the rest of the system."

**Q5.** *"Did you work alone or with a team, and how did you manage communication?"*  
"I led the project from design to deployment, but collaborated closely with sales and finance teams to gather requirements and validate outputs.  
Clear documentation and regular feedback sessions were key to success."

---

## 3. Common Data Engineer Interview Questions

**Q6.** *"Tell me about a time when you improved a data pipeline or ETL process."*  
"I optimized an existing ETL process by restructuring transformation logic and introducing incremental data loads.  
This reduced processing time by 60% and minimized system resource usage."

**Q7.** *"How do you approach designing a data model for scalability?"*  
"I start by understanding the business requirements and expected growth in data volume.  
I then normalize where needed for integrity, and denormalize selectively for performance, ensuring indexes and partitioning strategies are in place."

**Q8.** *"Can you describe a situation where you had to deal with incomplete or inconsistent data?"*  
"In one project, data from external sources often arrived with missing fields.  
I implemented pre-processing validation scripts to fill gaps from reference datasets and flagged any remaining anomalies for manual review."

**Q9.** *"Whatâ€™s your process for translating business requirements into technical solutions?"*  
"I work closely with stakeholders to define clear requirements, map them to technical specifications, and validate through prototypes before full-scale implementation."

**Q10.** *"How do you stay updated with new data engineering tools and technologies?"*  
"I follow industry publications, participate in professional communities, and regularly test new tools in small-scale proof-of-concepts before adopting them in production."

---

## 4. Acceptable Humility Workarounds

- **If feeling insecure at the start:**  
"While I can communicate in English, I donâ€™t have daily conversation practice, so sometimes I need an extra second to express an idea.  
Thatâ€™s why Iâ€™ve been taking structured speaking lessons to improve my fluency, and Iâ€™m already seeing progress."

- **If stuck mid-response:**  
"Sorry, I just need a quick moment to phrase this correctly â€” English is not my first language, but Iâ€™m actively improving it.  
I prefer to take a second rather than give you the wrong answer."

- **If asked directly about language skills:**  
"I believe communication is key, and thatâ€™s why Iâ€™m investing in my English speaking skills through a dedicated course.  
Iâ€™m confident my technical expertise and problem-solving abilities will allow me to deliver results while I continue improving my fluency."
